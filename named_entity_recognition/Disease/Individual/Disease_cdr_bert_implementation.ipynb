{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c715b838-828b-4c7a-9701-259135003070",
     "showTitle": true,
     "title": "Importing libraries"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import keras\n",
    "import nltk\n",
    "from nltk.tokenize.util import align_tokens\n",
    "nltk.download('punkt')\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dff9cf1-3c07-41c3-8212-0b97b7bd8d40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f69da65-5618-4d3f-8565-067668d41cb1",
     "showTitle": true,
     "title": "Read the data"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"/dbfs/FileStore/Chanchal/Datasets/cdr.json\", \"r\") as f:\n",
    "  data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc968af-b6cc-4863-80f5-7ca46e9cd9ca",
     "showTitle": true,
     "title": "Transforming data to Label-O format"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\nError\n"
     ]
    }
   ],
   "source": [
    "string_labels = []\n",
    "for item in data:\n",
    "  text = item['Text']\n",
    "  id= item['id']\n",
    "  sentences_split = nltk.word_tokenize(item[\"Text\"])\n",
    "  # print(sentences_split)\n",
    "  try:\n",
    "      token_spans = align_tokens(sentences_split,text)\n",
    "  except:\n",
    "      print(\"Error\")\n",
    "      # print(token_spans)\n",
    "  for i in range(len(token_spans)):\n",
    "      token_in_annotation = False\n",
    "      for annotation in item['annotation']:\n",
    "          if int(annotation[\"start\"])<= token_spans[i][0] and int(annotation['end'])>=token_spans[i][1]:\n",
    "              string_labels.append((text[token_spans[i][0]:token_spans[i][1]],annotation['type']))\n",
    "              token_in_annotation = True\n",
    "      if(token_in_annotation==False):\n",
    "          string_labels.append((text[token_spans[i][0]:token_spans[i][1]],'O'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51277bce-9248-4c50-a69c-1fbcc2e81584",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#addding data to dataframe\n",
    "df = pd.DataFrame(string_labels, columns =['token', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "840d5511-c087-48a6-8ccf-29fb01a27bc8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tricuspid</td>\n",
       "      <td>Disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>valve</td>\n",
       "      <td>Disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>regurgitation</td>\n",
       "      <td>Disease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lithium</td>\n",
       "      <td>Chemical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325682</th>\n",
       "      <td>challenge</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325683</th>\n",
       "      <td>tests</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325684</th>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325685</th>\n",
       "      <td>rodents</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325686</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>325687 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                token     label\n",
       "0           Tricuspid   Disease\n",
       "1               valve   Disease\n",
       "2       regurgitation   Disease\n",
       "3                 and         O\n",
       "4             lithium  Chemical\n",
       "...               ...       ...\n",
       "325682      challenge         O\n",
       "325683          tests         O\n",
       "325684             in         O\n",
       "325685        rodents         O\n",
       "325686              .         O\n",
       "\n",
       "[325687 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9cf9e6b-4ab5-4e36-841a-31b603305f6f",
     "showTitle": true,
     "title": "Frequency of labels"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tags: 3\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "O           290001\n",
       "Disease      20104\n",
       "Chemical     15582\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of tags: {}\".format(len(df.label.unique())))\n",
    "frequencies = df.label.value_counts()\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f0e274-f5ff-4196-a704-906d25f32633",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_labels = ['Disease','O']\n",
    "df = df[df['label'].isin(target_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52995427-e906-45e2-aa18-dc9b1bb7503e",
     "showTitle": true,
     "title": "Splitting long paragraph based on full stop to input in bert"
    }
   },
   "outputs": [],
   "source": [
    "df['group'] = (df['token'] == '.').cumsum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de5ecb2a-adde-47d9-bc3c-da46e1cb08f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = df.groupby('group').agg({'token': ' '.join, 'label': ' '.join}).reset_index(drop=True)\n",
    "\n",
    "new_df.rename(columns = {'token':'text', 'label':'labels'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e773e024-8d0f-442a-8803-5ae8492e784d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split labels based on whitespace and turn them into a list\n",
    "labels = [i.split() for i in new_df['labels'].values.tolist()]\n",
    "\n",
    "# Check how many labels are there in the dataset\n",
    "unique_labels = set()\n",
    "\n",
    "for lb in labels:\n",
    "  [unique_labels.add(i) for i in lb if i not in unique_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66c6d673-c468-4600-b937-a34c7ae18a60",
     "showTitle": true,
     "title": "Mapping label to ids"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Chemical': 0, 'O': 1}\n"
     ]
    }
   ],
   "source": [
    "# Map each label into its id representation and vice versa\n",
    "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
    "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
    "print(labels_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354fd744-9678-4332-8a60-af95b20056ce",
     "showTitle": true,
     "title": "Importing Tokenizer"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44c604b936f44debae4f61330f15dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9127de3a56d6455d823a0fdb8d7fefb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11a74ac11b44da182951e504f09f493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3034b8bb04a4526a1b1ee18c4c0eb3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce3dec2-08a2-452b-b2df-abd5f3a07d71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "txt=new_df['text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73a62a59-41d1-4811-9af2-412a8685ac7c",
     "showTitle": true,
     "title": "Tokenizing the texts"
    }
   },
   "outputs": [],
   "source": [
    "text_tokenized = tokenizer(txt, padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3989e4fa-bc7f-485d-ac6e-1a352a0c4491",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_ids=text_tokenized['input_ids']\n",
    "attention_masks=text_tokenized['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c1c820-598c-472c-993f-eb4ed7d6b434",
     "showTitle": true,
     "title": "Balancing the labels according to the tokenized text"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def align_label(texts, labels):\n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "    label_all_tokens=True\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]])\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3611de41-f574-4941-a379-f199dfc1b66b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lb = [i.split() for i in new_df['labels'].values.tolist()]\n",
    "\n",
    "txt = new_df['text'].values.tolist()\n",
    "\n",
    "new_label =[align_label(i,j) for i,j in zip(txt, lb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be574b6-558b-492c-92e0-d4123e52e898",
     "showTitle": true,
     "title": "Converting inputs for bert in the form of Tensors"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-328077034943194>:5\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m pt_input_ids \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(\u001B[38;5;28mlist\u001B[39m(input_ids), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
       "\u001B[1;32m      3\u001B[0m pt_attention_masks \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(\u001B[38;5;28mlist\u001B[39m(attention_masks), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
       "\u001B[0;32m----> 5\u001B[0m pt_labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(new_label, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'new_label' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-328077034943194>:5\u001B[0m\n\u001B[1;32m      1\u001B[0m pt_input_ids \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(\u001B[38;5;28mlist\u001B[39m(input_ids), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m      3\u001B[0m pt_attention_masks \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(\u001B[38;5;28mlist\u001B[39m(attention_masks), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m pt_labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(new_label, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\n\u001B[0;31mNameError\u001B[0m: name 'new_label' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'new_label' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pt_input_ids = torch.stack(list(input_ids), dim=0)\n",
    "\n",
    "pt_attention_masks = torch.stack(list(attention_masks), dim=0)\n",
    "\n",
    "pt_labels = torch.tensor(new_label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5355644e-4543-465d-b674-c228660d2f36",
     "showTitle": true,
     "title": "Splitting data into train, val and test dataset"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8,162 training samples\n2,720 validation samples\n2,722 testing samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "import random\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(pt_input_ids, pt_attention_masks, pt_labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "# df_train = int(0.8 * len(dataset))\n",
    "# df_dev=int(.9 * len(dataset))\n",
    "# df_test=int(len(dataset) - (df_train+ df_dev))\n",
    "# # # val_size = len(dataset) - train_size\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.6 * total_size)\n",
    "val_size = int(0.20 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "print('{:>5,} testing samples'.format(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "221fbaf8-cf81-4815-bad7-e84f7b122190",
     "showTitle": true,
     "title": "Converting dataset in the form of Data loader"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size )\n",
    "\n",
    "validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = batch_size   )\n",
    "\n",
    "test_dataloader=DataLoader(train_dataset, sampler = SequentialSampler(test_dataset), batch_size = batch_size )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695b3985-57c5-425e-963b-6ee446374a5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73240e4b-bbd4-425d-a7e0-636837b3c362",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a259d498-7146-4586-8a81-ac604a70bc41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529ea14df9b04bd59ac0f14e65b82ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import BertForTokenClassification, AdamW, BertConfig\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=len(labels_to_ids))\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f93c23-642e-4263-abed-9cd894a776aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# defining the optimizer\n",
    "lr=1e-05\n",
    "epochs=4\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                   # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c2bf303-6dda-4b50-9639-90cadf280d0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00d0efca-2405-4c27-bfe8-743def99ef09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce15f36b-f14b-4aba-a56c-505d9fb81cad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e8dd80-281b-4dbb-8afa-81ff3eee7386",
     "showTitle": true,
     "title": "Training the model"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\nTraining...\n  Batch    40  of    511.    Elapsed: 0:00:54.\n  Batch    80  of    511.    Elapsed: 0:01:48.\n  Batch   120  of    511.    Elapsed: 0:02:44.\n  Batch   160  of    511.    Elapsed: 0:03:41.\n  Batch   200  of    511.    Elapsed: 0:04:39.\n  Batch   240  of    511.    Elapsed: 0:05:37.\n  Batch   280  of    511.    Elapsed: 0:06:34.\n  Batch   320  of    511.    Elapsed: 0:07:32.\n  Batch   360  of    511.    Elapsed: 0:08:29.\n  Batch   400  of    511.    Elapsed: 0:09:27.\n  Batch   440  of    511.    Elapsed: 0:10:24.\n  Batch   480  of    511.    Elapsed: 0:11:22.\n\n  Average training loss: 0.11\n  Training epcoh took: 0:12:05\nRunning Validation...\n  Average validation loss: 0.08\n======== Epoch 2 / 3 ========\nTraining...\n  Batch    40  of    511.    Elapsed: 0:00:58.\n  Batch    80  of    511.    Elapsed: 0:01:56.\n  Batch   120  of    511.    Elapsed: 0:02:54.\n  Batch   160  of    511.    Elapsed: 0:03:53.\n  Batch   200  of    511.    Elapsed: 0:04:51.\n  Batch   240  of    511.    Elapsed: 0:05:49.\n  Batch   280  of    511.    Elapsed: 0:06:47.\n  Batch   320  of    511.    Elapsed: 0:07:45.\n  Batch   360  of    511.    Elapsed: 0:08:44.\n  Batch   400  of    511.    Elapsed: 0:09:42.\n  Batch   440  of    511.    Elapsed: 0:10:39.\n  Batch   480  of    511.    Elapsed: 0:11:36.\n\n  Average training loss: 0.06\n  Training epcoh took: 0:12:19\nRunning Validation...\n  Average validation loss: 0.07\n======== Epoch 3 / 3 ========\nTraining...\n  Batch    40  of    511.    Elapsed: 0:00:57.\n  Batch    80  of    511.    Elapsed: 0:01:54.\n  Batch   120  of    511.    Elapsed: 0:02:52.\n  Batch   160  of    511.    Elapsed: 0:03:49.\n  Batch   200  of    511.    Elapsed: 0:04:46.\n  Batch   240  of    511.    Elapsed: 0:05:45.\n  Batch   280  of    511.    Elapsed: 0:06:42.\n  Batch   320  of    511.    Elapsed: 0:07:39.\n  Batch   360  of    511.    Elapsed: 0:08:37.\n  Batch   400  of    511.    Elapsed: 0:09:35.\n  Batch   440  of    511.    Elapsed: 0:10:32.\n  Batch   480  of    511.    Elapsed: 0:11:29.\n\n  Average training loss: 0.03\n  Training epcoh took: 0:12:12\nRunning Validation...\n  Average validation loss: 0.07\nTotal training took 0:41:20 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlflow.start_run()\n",
    "loss_values = []\n",
    "val_loss_vaues=[]\n",
    "total_t0 = time.time()\n",
    "for epoch in range(epochs):\n",
    "  print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
    "  print('Training...')\n",
    "  # Measure how long the training epoch takes.\n",
    "  training_stats = []\n",
    "  model.train()\n",
    "  total_loss = 0\n",
    "  t0 = time.time()\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss, logits = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask, \n",
    "                            labels=b_labels).to_tuple()\n",
    "    total_loss += loss.item()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "  avg_train_loss = total_loss / len(train_dataloader)\n",
    "  loss_values.append(avg_train_loss)\n",
    "  # Measure how long this epoch took.\n",
    "  training_time = format_time(time.time() - t0)\n",
    "  \n",
    "  print(\"\")\n",
    "  print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "  # print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "  print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "  # Validation\n",
    "  print(\"Running Validation...\")\n",
    "\n",
    "\n",
    "  t0 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "  model.eval()\n",
    "  total_eval_loss = 0\n",
    "  val_predictions = []\n",
    "  for batch in validation_dataloader:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    with torch.no_grad():\n",
    "      loss, logits = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask,labels=b_labels).to_tuple()\n",
    "    total_eval_loss += loss.item()\n",
    "    predicted_labels = torch.argmax(logits, dim=2)\n",
    "    val_predictions.extend(predicted_labels.detach().cpu().numpy())\n",
    "\n",
    "  avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "  val_loss_vaues.append(avg_val_loss)\n",
    "  validation_time = format_time(time.time() - t0)\n",
    "\n",
    "  print(\"  Average validation loss: {0:.2f}\".format(avg_val_loss))\n",
    "  mlflow.log_param(\"lr\", lr)\n",
    "  mlflow.log_metric(\"epoch\", epoch + 1)\n",
    "  mlflow.log_metric(\"average_train_loss\", avg_train_loss, step=epoch+1)\n",
    "  mlflow.log_metric(\"average_val_loss\", avg_val_loss,step=epoch+1)\n",
    "  \n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a652912-e137-4e09-8034-866c21d2666e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# print('Predicting labels for {:,} test sentences...'.format(len(pt_input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in test_dataloader:\n",
    "  # Add batch to GPU\n",
    "  b_input_ids = batch[0].to(device)\n",
    "  b_input_mask = batch[1].to(device)\n",
    "  b_labels = batch[2].to(device)\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "  logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "795eafac-4a17-4666-899f-024ac1729db2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After flattening the batches, the predictions have shape:\n     (2722, 512, 2)\n\nAfter choosing the highest scoring label for each token:\n     (2722, 512)\n\nAfter flattening the sentences, we have predictions:\n     (1393664,)\nand ground truth:\n     (1393664,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# First, combine the results across the batches.\n",
    "all_predictions = np.concatenate(predictions, axis=0)\n",
    "all_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "print(\"After flattening the batches, the predictions have shape:\")\n",
    "print(\"    \", all_predictions.shape)\n",
    "\n",
    "# Next, let's remove the third dimension (axis 2), which has the scores\n",
    "# for all 18 labels. \n",
    "\n",
    "# For each token, pick the label with the highest score.\n",
    "predicted_label_ids = np.argmax(all_predictions, axis=2)\n",
    "\n",
    "print(\"\\nAfter choosing the highest scoring label for each token:\")\n",
    "print(\"    \", predicted_label_ids.shape) \n",
    "\n",
    "\n",
    "# Eliminate axis 0, which corresponds to the sentences.\n",
    "predicted_label_ids = np.concatenate(predicted_label_ids, axis=0)\n",
    "all_true_labels = np.concatenate(all_true_labels, axis=0)\n",
    "\n",
    "print(\"\\nAfter flattening the sentences, we have predictions:\")\n",
    "print(\"    \", predicted_label_ids.shape)\n",
    "print(\"and ground truth:\")\n",
    "print(\"    \", all_true_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3294c1e2-234b-4189-9c40-2ce9771f2df6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering out `null` tokens, length = 1,393,664\n After filtering out `null` tokens, length = 61,764\n"
     ]
    }
   ],
   "source": [
    "# Construct new lists of predictions which don't include any null tokens.\n",
    "real_token_predictions = []\n",
    "real_token_labels = []\n",
    "\n",
    "# For each of the input tokens in the dataset...\n",
    "for i in range(len(all_true_labels)):\n",
    "\n",
    "    # If it's not a token with a null label...\n",
    "    if not all_true_labels[i] == -100:\n",
    "        \n",
    "        # Add the prediction and the ground truth to their lists.\n",
    "        real_token_predictions.append(predicted_label_ids[i])\n",
    "        real_token_labels.append(all_true_labels[i])\n",
    "\n",
    "print(\"Before filtering out `null` tokens, length = {:,}\".format(len(all_true_labels)))\n",
    "print(\" After filtering out `null` tokens, length = {:,}\".format(len(real_token_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c5d8f15-8ab8-4ef7-8e69-6083d5537561",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# f1 = f1_score(real_token_labels, real_token_predictions, average='micro')\n",
    "labels = [ids_to_labels[id.item()] for id in real_token_labels]\n",
    "predictions = [ids_to_labels[id.item()] for id in real_token_predictions]\n",
    "\n",
    "# print (\"F1 score: {:.2%}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f231320e-46a6-4309-b9db-31bcc3f1d658",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# precision= precision_score(real_token_labels, real_token_predictions, average='micro')\n",
    "# recall=recall_score(real_token_labels, real_token_predictions, average='micro')\n",
    "# # precision_score(labels, predictions, average = \"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a418c3e9-d0ca-4abf-af31-21fed67e588c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n\n     Disease       0.96      0.93      0.94      4151\n           O       1.00      1.00      1.00     57613\n\n    accuracy                           0.99     61764\n   macro avg       0.98      0.96      0.97     61764\nweighted avg       0.99      0.99      0.99     61764\n\n"
     ]
    }
   ],
   "source": [
    "# from seqeval.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc7de854-df01-4e8b-8dbc-b4032944d6ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "report = classification_report(labels, predictions, output_dict=True)\n",
    "\n",
    "df = pd.DataFrame(report).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e57b8b8-4b55-4ab6-b0da-146957ea5cbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"/dbfs/FileStore/Chanchal/logs/cdr_chemical_.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d778fb59-a94b-4ee2-abdd-75d654f7d4da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df2"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Disease_cdr_bert_implementation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
